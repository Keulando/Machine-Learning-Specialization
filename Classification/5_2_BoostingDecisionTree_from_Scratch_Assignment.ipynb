{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting a decision stump\n",
    "In this homework you will implement your own boosting module.\n",
    "\n",
    "Brace yourselves! This is going to be a fun and challenging assignment.\n",
    "\n",
    "Use SFrames to do some feature engineering.\n",
    "Train a boosted ensemble of decision-trees (gradient boosted trees) on the lending club dataset.\n",
    "Predict whether a loan will default along with prediction probabilities (on a validation set).\n",
    "Evaluate the trained model and compare it with a baseline.\n",
    "Find the most positive and negative loans using the learned model.\n",
    "Explore how the number of trees influences classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:52.835735Z",
     "start_time": "2018-03-29T15:40:52.239151Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:52.845296Z",
     "start_time": "2018-03-29T15:40:52.838744Z"
    }
   },
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length'        # number of years of employment\n",
    "           ]\n",
    "target= 'safe_loans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:55.937219Z",
     "start_time": "2018-03-29T15:40:52.847800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Keulando\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (19,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "loans=pd.read_csv('lending-club-data.csv')\n",
    "\n",
    "#modify target column\n",
    "\n",
    "loans['safe_loans']=loans['bad_loans'].apply(lambda x: 1 if x==0 else -1)\n",
    "loans.drop('bad_loans', axis=1, inplace=True)\n",
    "loans=loans[[target]+ features]\n",
    "loans=pd.get_dummies(loans)\n",
    "\n",
    "features_dummy=list(loans.columns[1:])\n",
    "\n",
    "train_idx=pd.read_json('module-8-assignment-2-train-idx.json', typ='series')\n",
    "test_idx=pd.read_json('module-8-assignment-2-test-idx.json', typ='series')\n",
    "\n",
    "train=loans.iloc[train_idx].reset_index(drop=True)\n",
    "test=loans.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "X_train=train.iloc[:,1:]\n",
    "y_train=train.iloc[:,0]\n",
    "\n",
    "X_test=test.iloc[:,1:]\n",
    "y_test=test.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:55.952256Z",
     "start_time": "2018-03-29T15:40:55.939714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade_A',\n",
       " 'grade_B',\n",
       " 'grade_C',\n",
       " 'grade_D',\n",
       " 'grade_E',\n",
       " 'grade_F',\n",
       " 'grade_G',\n",
       " 'term_ 36 months',\n",
       " 'term_ 60 months',\n",
       " 'home_ownership_MORTGAGE',\n",
       " 'home_ownership_OTHER',\n",
       " 'home_ownership_OWN',\n",
       " 'home_ownership_RENT',\n",
       " 'emp_length_1 year',\n",
       " 'emp_length_10+ years',\n",
       " 'emp_length_2 years',\n",
       " 'emp_length_3 years',\n",
       " 'emp_length_4 years',\n",
       " 'emp_length_5 years',\n",
       " 'emp_length_6 years',\n",
       " 'emp_length_7 years',\n",
       " 'emp_length_8 years',\n",
       " 'emp_length_9 years',\n",
       " 'emp_length_< 1 year',\n",
       " 'emp_length_n/a']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted decision trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:55.974783Z",
     "start_time": "2018-03-29T15:40:55.955232Z"
    }
   },
   "outputs": [],
   "source": [
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    #labels_in_node.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
    "    weighted_mistakes_all_negative = total_weight_positive\n",
    "    #print('weighted mistakes all negative  {}'.format(weighted_mistakes_all_negative))\n",
    "    \n",
    "    total_weight_negative= sum(data_weights[labels_in_node == -1])\n",
    "    weighted_mistakes_all_positive = total_weight_negative\n",
    "    #print('weighted mistakes all  positive {}'.format(weighted_mistakes_all_positive))\n",
    "    \n",
    "    #if weighted_mistakes_all_positive <= weighted_mistakes_all_negative:\n",
    "     #   return (weighted_mistakes_all_positive,+1)\n",
    "    #else:\n",
    "     #   return ( weighted_mistakes_all_negative, -1)\n",
    "    if weighted_mistakes_all_negative <= weighted_mistakes_all_positive:\n",
    "        return (weighted_mistakes_all_negative, -1)\n",
    "    else:\n",
    "        return ( weighted_mistakes_all_positive, +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:55.991831Z",
     "start_time": "2018-03-29T15:40:55.977292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed\n"
     ]
    }
   ],
   "source": [
    "ex_labels=pd.Series([-1,-1,1,1,1])\n",
    "ex_weights= pd.Series([1.,2.,.5,1.,1.])\n",
    "if intermediate_node_weighted_mistakes(ex_labels,ex_weights)==(2.5,-1):\n",
    "    print('test passed')\n",
    "else:\n",
    "    print('failsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:56.017899Z",
     "start_time": "2018-03-29T15:40:55.994838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_labels1=pd.Series([1,1,1,1,1])\n",
    "ex_weights1=pd.Series([1.,2.,.5,1.,1.])\n",
    "intermediate_node_weighted_mistakes(ex_labels1,ex_weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:56.074050Z",
     "start_time": "2018-03-29T15:40:56.021410Z"
    }
   },
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    \n",
    "    target_values = data[target]\n",
    "    best_feature  = None #keep track of best feature\n",
    "    best_error    = float('+inf') #keep track best error\n",
    "    \n",
    "    num_data_points=float(len(data))\n",
    "    \n",
    "    for feature in features:\n",
    "        left_split  = data[data[feature]==0]\n",
    "        right_split = data[data[feature]==1]\n",
    "        #left_split  = data[feature]==0\n",
    "        left_data_weights = data_weights[left_split.index]\n",
    "        right_data_weights = data_weights[right_split.index]\n",
    "        #left_data_weights = data_weights[data[feature]==0]\n",
    "        #right_data_weights = data_weights[data[feature]==1]\n",
    "        #left_data_weights= left_split[data_weights]\n",
    "        #left_data_weights = data_weights[left_split]\n",
    "        #right_data_weights = data_weights[right_split]\n",
    "        \n",
    "        \n",
    "        left_weighted_mistakes, left_class  = intermediate_node_weighted_mistakes(left_split[target], left_data_weights)\n",
    "        right_weighted_mistakes, right_class = intermediate_node_weighted_mistakes(right_split[target], right_data_weights)\n",
    "        \n",
    "        #error = (left_weighted_mistakes + right_weighted_mistakes)/(sum(left_data_weights) + sum(right_data_weights))\n",
    "        error = (left_weighted_mistakes + right_weighted_mistakes)/sum(data_weights)\n",
    "\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error   = error\n",
    "   \n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:56.722812Z",
     "start_time": "2018-03-29T15:40:56.077059Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed\n"
     ]
    }
   ],
   "source": [
    "ex_weights2=pd.Series(len(train)*[1.5])\n",
    "if best_splitting_feature(train, features_dummy, target, ex_weights2) == 'term_ 36 months':\n",
    "    print('test passed')\n",
    "else:\n",
    "    print(\"fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:56.731296Z",
     "start_time": "2018-03-29T15:40:56.724778Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values, data_weights):\n",
    "    \n",
    "    leaf= {'splitting feature': None,\n",
    "          'is_leaf':True}\n",
    "    \n",
    "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
    "    leaf['prediction'] = best_class\n",
    "    \n",
    "    return leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:56.837077Z",
     "start_time": "2018-03-29T15:40:56.733803Z"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    #target_values\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"Subtree, depth = {} ({} data points).\".format(current_depth, len(target_values)))\n",
    "    \n",
    "    # Stopping condition 1. Error is 0.\n",
    "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "        print(\"Stopping condition 1 reached.\")                \n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # Stopping condition 2. No more features.\n",
    "    if remaining_features == []:\n",
    "        print(\"Stopping condition 2 reached.\")                \n",
    "        return create_leaf(target_values, data_weights)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth > max_depth:\n",
    "        print(\"Reached maximum depth. Stopping for now.\")\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # If all the datapoints are the same, splitting_feature will be None. Create a leaf\n",
    "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    remaining_features.remove(splitting_feature)\n",
    "        \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    left_data_weights = data_weights[data[splitting_feature] == 0]\n",
    "    right_data_weights = data_weights[data[splitting_feature] == 1]\n",
    "    \n",
    "    print(\"Split on feature {}. (left: {}, right: {})\".format(splitting_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(left_split[target], data_weights)\n",
    "    if len(right_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(right_split[target], data_weights)\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = weighted_decision_tree_create(\n",
    "        left_split, remaining_features, target, left_data_weights, current_depth + 1, max_depth)\n",
    "    right_tree = weighted_decision_tree_create(\n",
    "        right_split, remaining_features, target, right_data_weights, current_depth + 1, max_depth)\n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:56.852619Z",
     "start_time": "2018-03-29T15:40:56.840085Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:58.224307Z",
     "start_time": "2018-03-29T15:40:56.855161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term_ 36 months. (left: 9223, right: 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Split on feature grade_A. (left: 9122, right: 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (9122 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (101 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Split on feature grade_D. (left: 23300, right: 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (23300 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (4701 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "test passed\n"
     ]
    }
   ],
   "source": [
    "ex1= pd.Series([1.0 for i in range(len(train))])\n",
    "small_tree= weighted_decision_tree_create(train, features_dummy,target, ex1, max_depth=2)\n",
    "\n",
    "if count_nodes(small_tree)==7:\n",
    "    print('test passed')\n",
    "else:\n",
    "    print('test failed')\n",
    "    print('number of nodes found; {}'.format(count_nodes(small_tree)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:58.245323Z",
     "start_time": "2018-03-29T15:40:58.227274Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate=False):\n",
    "    if tree['is_leaf']:\n",
    "        if annotate:\n",
    "            print('At leaft, predicting {}'.format(tree['prediction']))\n",
    "        return tree['prediction']\n",
    "    else:\n",
    "        #split on feature\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate:\n",
    "            print('split on {} = {}'.format(tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluating tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:58.261467Z",
     "start_time": "2018-03-29T15:40:58.248328Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_classificaton_error(tree, data):\n",
    "    #target='safe_loans'\n",
    "    prediction = data.apply(lambda x: classify(tree, x), axis=1)\n",
    "    mistakes = (prediction != data[target]).sum() / float(len(data))\n",
    "    return mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:58.599764Z",
     "start_time": "2018-03-29T15:40:58.263871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3981042654028436"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classificaton_error(small_tree, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example\n",
    "weight\n",
    "- 1 to the last 10 items\n",
    "- 1 to the first 10 items\n",
    "\n",
    "- and 0 to the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:58.708052Z",
     "start_time": "2018-03-29T15:40:58.602772Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Keulando\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Keulando\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "example_weights= pd.Series([1.]*10 + [0.]*(len(train) -20) + [1.]*10)\n",
    "example_weights = np.array(example_weights)\n",
    "#len(example_weights), len(train[target])\n",
    "#example_weights= pd.DataFrame(data=[1]*10 + [0]*(len(train) -20) + [1]*10)\n",
    "ex=train.safe_loans\n",
    "ex[:11]=1\n",
    "ex[10:-10]=0\n",
    "\n",
    "example_data_weights = np.ones(10*1).tolist() + [0.]*(len(train) - 20) + np.ones(1*10).tolist()\n",
    "example_data_weights = np.array(example_data_weights)\n",
    "\n",
    "ex2=pd.Series(len(train)* [1.5], index= train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:58.845416Z",
     "start_time": "2018-03-29T15:40:58.710559Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (9284 data points).\n"
     ]
    },
    {
     "ename": "IndexingError",
     "evalue": "Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-11710d82faaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m small_tree_subset_20 = weighted_decision_tree_create(test, features_dummy, target,\n\u001b[1;32m----> 2\u001b[1;33m                                                     ex2, max_depth=2)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-8636b47c2ab2>\u001b[0m in \u001b[0;36mweighted_decision_tree_create\u001b[1;34m(data, features, target, data_weights, current_depth, max_depth)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Stopping condition 1. Error is 0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mintermediate_node_weighted_mistakes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1e-15\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Stopping condition 1 reached.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_leaf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-4f3b8dbd88cd>\u001b[0m in \u001b[0;36mintermediate_node_weighted_mistakes\u001b[1;34m(labels_in_node, data_weights)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#labels_in_node.reset_index(drop=True, inplace=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtotal_weight_positive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabels_in_node\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mweighted_mistakes_all_negative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_weight_positive\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print('weighted mistakes all negative  {}'.format(weighted_mistakes_all_negative))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[1;34m(ax, key)\u001b[0m\n\u001b[0;32m   1937\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1938\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1939\u001b[1;33m             raise IndexingError('Unalignable boolean Series provided as '\n\u001b[0m\u001b[0;32m   1940\u001b[0m                                 \u001b[1;34m'indexer (index of the boolean Series and of '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1941\u001b[0m                                 'the indexed object do not match')\n",
      "\u001b[1;31mIndexingError\u001b[0m: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match"
     ]
    }
   ],
   "source": [
    "small_tree_subset_20 = weighted_decision_tree_create(test, features_dummy, target,\n",
    "                                                    ex2, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:11:00.900950Z",
     "start_time": "2018-03-29T15:11:00.889417Z"
    }
   },
   "source": [
    "### small_tree_subset_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T09:04:48.030509Z",
     "start_time": "2018-03-28T09:04:39.098Z"
    }
   },
   "source": [
    "### train[features_dummy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:58.845949Z",
     "start_time": "2018-03-29T15:40:52.366Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_classificaton_error(small_tree_subset_20, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:58.847107Z",
     "start_time": "2018-03-29T15:40:52.373Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:58.847924Z",
     "start_time": "2018-03-29T15:40:52.379Z"
    }
   },
   "outputs": [],
   "source": [
    "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
    "    alpha = pd.Series(np.ones(len(data)), index=data.index)\n",
    "    weights= []\n",
    "    tree_stumps = []\n",
    "    target_values = data[target]\n",
    "    \n",
    "    for t in range(num_tree_stumps):\n",
    "        print(\" ====================================\")\n",
    "        print(' Adaboost Iteration {}'.format(t))\n",
    "        print('======================================')\n",
    "        \n",
    "        # learn a decision tree stump, depth 1\n",
    "        tree_stump = weighted_decision_tree_create(data, features, target, data_weights=alpha, max_depth=1)\n",
    "        tree_stumps.append(tree_stump)\n",
    "        \n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x), axis=1)\n",
    "        \n",
    "        is_correct = (predictions == target_values)\n",
    "        is_wrong   = (predictions != target_values)\n",
    "        \n",
    "        #compute weighted error\n",
    "        #weighted_error= sum(alpha[is_wrong])/sum(alpha)\n",
    "        #weighted_error = is_wrong.sum()/ float(len(alpha))\n",
    "        weighted_error = round(float(sum(a for i,a in enumerate(alpha) if is_wrong[i]))/sum(alpha), 5)\n",
    "        #compute model coef uing wei error\n",
    "        weight= 0.5 * log((1- weighted_error)/weighted_error)\n",
    "        weights.append(weight)\n",
    "        \n",
    "        #adjust weights on data point\n",
    "        adjustment = is_correct.apply(lambda is_correct: exp(-weight) if is_correct else exp(weight))\n",
    "        \n",
    "        #scale alpha by multip by adjustements\n",
    "        alpha = (alpha* adjustment) / float(sum(alpha))\n",
    "        #print(alpha)\n",
    "        \n",
    "    return weights, tree_stumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T15:40:58.848925Z",
     "start_time": "2018-03-29T15:40:52.386Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "AdaB=adaboost_with_tree_stumps(train, features_dummy, target, num_tree_stumps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
